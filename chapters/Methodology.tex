\chapter{Methodology}\label{chapter:Methodology}
The following chapter explains in detail how the first four steps postulated in the book (\cite{Hyndman.2013}) are used for forecasting in this Interdisciplinary Project. Step five, the results and evaluation, is discussed in the next chapters.\newline
\section{Problem Definition}\label{section:Problem Definition}
According to the process, some general questions have to be solved before the forecast is started. For this purpose a stakeholder analysis is done and a SIPOC diagram is created (Table \ref{tab:sipoc}).

\begin{table}[h]
\centering
\caption{SIPOC diagram derived from the five basic steps}
\label{tab:sipoc}
\scalebox{0.75}{%
\begin{tabular}{|l|l|l|l|l|}
\hline
Supplier                                                                                    & Input                                       & Process                                                          & Output                                 & Customer                                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Emanuel Pallua (COO)\\ Sebastian Sondheimer (BI)\\ Stefan Rothlehner(CTO)\\ Sergej Krauze (CTO)\\ Operations Team\end{tabular} & \begin{tabular}[c]{@{}l@{}}Knowledge\\ Experience\\ Historic Database\end{tabular} & \begin{tabular}[c]{@{}l@{}}Preliminary Analysis\\ Choosing Models\\ Fitting Models\\ Forecasting\\ Evaluating\end{tabular} & \begin{tabular}[c]{@{}l@{}}Forecast\\ Evaluation\\ Result\end{tabular} & \begin{tabular}[c]{@{}l@{}}Emanuel Pallua\\ Sebastian Sondheimer\\ Operation Team\end{tabular} \\ \hline
\end{tabular}}
\end{table}
Emanuel Pallua is identified as the first stakeholder. He is Chief of Operations at VOLO and the one who requested the forecast. His requirement for the forecast is to decrease "waste". Waste is defined as idle times for drivers, like being at the restaurant too early or food losing freshness by lying at the restaurant for too long. Another goal is to save money by maximizing the utilization of drivers and thus decreasing the fleet size. In general, he wants a robust forecasting process which can be continuously improved and automated in the future for day to day usage. \newline
The second source and customer in one person is Sebastian Sondheimer who is with Business Intelligence. His aim is to optimize the whole process of VOLO. He supports the forecast process by adding his knowledge of the current key numbers of VOLO. He suggests using different performance factors for the forecast. This should include the preparation times of this current slot and day, of the slot in the last seven days and all orders of the last week as well as of the slot and all orders on this weekday of the last four weeks.\newline
As sources on the technical side, there are Stefan Rothlehner and Sergej Krauze, both Chief Technical Officers. Stefan Rothlehner is responsible for the backend database and delivers the data. The information of the orders is extracted from the PostgreSQL database which is hosted on the heroku server. Sergej Krauze who is responsible for the Traveling Salesman Algorithm has the requirement to have the forecast written in Java. Since his work is already in Java, the forecast can be integrated without much additional effort, once decided that it should be integrated at all.\newline
The last source and customer is the Operations Team since they are involved in the everyday business. They support the algorithm with their real life scenario knowledge. Their experience is that a 15 minute estimation works reasonably well. In their opinion, a decrease of one third, 33\%, of waste will make it worth to implement the algorithm. In order to have a benchmark for the results of the forecast, the error of their current assumptions will be calculated later.
\newline\newline\textbf{Problem Statement}\newline
The goal is to research a forecast for preparation times of meals. The approach should be simple and easy to implement together with the Traveling Salesman Algorithm. Reduction of waste should be significant over the current approach.
\section{Gathering Information}\label{section:Gathering Information}
The data for the forecast is taken from the PostgreSQL database of the backend. Every order from the beginning of VOLO in September 2014 has been processed and stored in this database. Each order has timestamps for each step of the delivery process. This way a rich set of historical data has piled up and is available. For this interdisciplinary project, two data samples are downloaded at different points in time, the Training Data Set and the Test Data Set. The Training Data Set is used to "train" the model and find potential relationships. It creates the first forecast. The result of the forecast is then compared to the result of the Test Data Set. This gives information about the prediction abilities of the model. When the two results are significantly different, the predictive relationships do not hold. As the Test Data Set is bigger than the Training Set, it also gives an indication how the results may improve as more data becomes available. The two data sets were dumped on the 26th of March and the 29th of April and contain 3034 and 4973 orders respectively. The downloaded data sets were saved in comma separated values files, known as csv files. Since VOLO was bought and moved to Berlin in April 2015, the company has grown very fast and with it came different problems in operations, e.g. too little expertise of the city's operations team or drivers in new cities. In order to have a forecast for a "working" system, only data from the Munich times was used, when the process was completely supervised by people who had done this for a longer time. New data can be used again when the operations have risen to normality, as observed before the move.
\newpage
The raw data has some weaknesses. The biggest one is the size of the gathered data, i.e. the lack of data items for some of the more granular categories defined later. Forecasts need big amounts of information to generate a meaningful result. Since some restaurants have too few orders or a lot of bad data for the preparation times, additional expertise has to be put into the forecast. This expertise is taken from the Operation Team. They have to deal with orders on a daily basis and right now have to "forecast" the point in time the driver has to be at the restaurant on their own. In their experience, an estimation of around 15 minutes to prepare an order is a more or less accurate guess for most restaurants except for some which are known for their unpredictability.\newline
In order to use the datasets from the csv files, they have to be parsed into objects in the Java program. For this purpose a csv parser library is used. The library reads the csv file and matches the orders from the database to the OrderModel.class of the code. Not all attributes of the database are used, only the ones related to the forecast are picked from the information. Since there is no tracking in the restaurant for the preparation time per se, it was decided to take the time interval from the point in time at which the restaurant receives the notification for the order until the driver leaves the restaurant. In the process of volo, the printer in the restaurant prints the receipt when the driver confirms her assignment to this delivery, which is saved in the database as \texttt{accepted\_at}. The timestamp of the driver leaving the restaurant is saved in \texttt{delivery\_started\_at}. Since it is not the task to figure out the exact preparation time but rather the time between when the order is sent to the restaurant and when the driver can pick it up,  these two time stamps can be used, so the clock for the preparation time starts ticking on \texttt{accepted\_at} and stops at \texttt{delivery\_started}.
\section{Preliminary Analysis}\label{section:Preliminary Analysis}
There are many factors which can influence the preparation times of the restaurant. Some are of external nature, some internal. The weather or season can have an effect or if a cook is ill.\newline
Marketing campaigns may boost orders short term, which can influence the results, but this has not happened in the recorded time frame.\newline
In order to discover these patterns and get a feeling for the data the preliminary analysis is done.
\subsection{Raw Data Cleanup}\label{subsection:Raw Data Cleanup}
First of all, the data has to be cleaned and a rough overview for the dimensions of the data gained. This is done to get a feeling of the time frame a restaurant usually needs to prepare an order. It provides an estimation of what to expect and clue to whether the result of a forecast is totally unrealistic or pretty close. In addition to the average time analysis, a visualization of the data is done. This is done in a time series plot on which it is very easy to see anomalies or patterns of the raw data.\newline

\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{images/0000_raw_3035.png}
\caption{Observations of preparation time of all orders without any data clean up}
\label{fig:0000_raw_3035}
\end{center}
\end{figure}

As the first step of the raw data cleanup, the training data is put into a time series graph with orders plotted with their preparation times (Fig. \ref{fig:0000_raw_3035}). It still contains unfinished orders and orders which have corrupted or missing timestamps. These orders cannot be transformed correctly and receive a total time of "-1" since their \texttt{accepted\_at} timestamp is not usable. In case the orders \texttt{delivery\_started\_at} timestamp is before the \texttt{accepted\_at} timestamp a negative value is visible. These events unrealistically reduce the average preparation time to 12 minutes and can be easily spotted in the time series graph. In order to get realistic results, these flaws in the data set have to be removed. This is done by ignoring all values which are below 0 minutes in the forecast. This action increases the average preparation time to 16 minutes which is much higher than the first result but free of misleading values. The data set which, in contrast to Figure \ref{fig:0000_raw_3035}, does not contain any negative values, is visualized in Figure \ref{fig:0000_invalid_2204}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{images/0000_invalid_2204.png}
\caption{Observations of preparation time of all orders without invalid values}
\label{fig:0000_invalid_2204}
\end{center}
\end{figure}

\newpage After removing the obviously invalid and corrupted orders the data looks a lot more consistent. The next problem are the orders which were finished long after they have been started. It has to be assumed that these times were either caused by bugged software or human error in the process. These data points are removed from the data set. For this purpose the Grubbs training for outliers is applied to the dataset and all values that seem not to come from a normally distributed population are removed (\cite{Grubbs}). Figure \ref{fig:0000_grubbed_2201} shows how this action influences the graph. The change can be noted in the scale of the diagram, which is now much more granular. This results in an average preparation time of 15 minutes which is the same result as the operations team suggest to use as a basis.

\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{images/0000_grubbed_2201.png}
\caption{Observations of preparation time of all orders without invalid values and outliers}
\label{fig:0000_grubbed_2201}
\end{center}
\end{figure}

All these steps from the clean up of the data are also applied to the Test Data Set which is used later.

\subsection{Identifying Patterns}
Now, as only the usable dataset is extracted from the input data, it can be analyzed for patterns, trends or similarities. For this purpose a time component has to be introduced into the graph since putting order after order does not reveal patterns.
\subsubsection{Daily Patterns}
As a first try, orders are summed up by day and visualized in Figure \ref{fig:0000_byDay_97}  by average preparation time. The most obvious information that can be extracted from the diagram is that the average preparation time varies by up to 19 minutes.

\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{images/0000_byDay_97.png}
\caption{Average preparation time per day}
\label{fig:0000_byDay_97}
\end{center}
\end{figure}


No clear trend or pattern can be observed in a day by day time axis diagram (Figure \ref{fig:0000_byDay_97}). In order to inspect behaviour of groups of events in a given category as opposed to single events on the time axis, box and whisker diagrams are created from the data. It shows the median and average preparation time as line and point inside the box. The box represents the upper and lower quartile of preparation times into which 50 \% of the data falls while the red whiskers visualize the entire population without outliers. Red circles and a red triangle are shown in case outliers are out of scale of the diagrams visualized range. The purpose of this kind of graph is to give an overview of the set of values of the preparation time and the boundaries most of the orders are located in. The data was analysed by different categories and plotted as box whisker graphs.\newline
\subsubsection{Week Patterns}
The first diagram is Figure \ref{fig:0000_boxWhisker_week_2129} which is on a week basis. It was created to see if there is a trend over time as the company expertise grows. The only observation that can be made is that after fluctuating preparation times in the first weeks it gets more constant towards the end but there is no clear trend.

\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{images/0000_boxWhisker_week_2129.png}
\caption{Box and whisker diagram on week basis}
\label{fig:0000_boxWhisker_week_2129}
\end{center}
\end{figure}

\subsubsection{Slot Patterns}
Since no conclusion can be drawn from week analysis, a box whisker diagram for each slot is created (Figure \ref{fig:0000_boxWhisker_slot_2129}). The day is divided into three slots, the big meals, lunch and dinner, as well as the time between these, which is not as busy as the meal times. The diagram shows no real difference in preparation times between the slots. A more in depth analysis has to be done, combining slots with other time categories (see below).

\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{images/0000_boxWhisker_slot_2129.png}
\caption{Box and whisker diagram on slot basis}
\label{fig:0000_boxWhisker_slot_2129}
\end{center}
\end{figure}

\subsubsection{Slot and Weekday Patterns}
Since weekdays can have strong differences in terms of load for the restaurant, a combination of weekday and slot needs to be analysed separately (Figure \ref{fig:0000_boxWhisker_weekdaySlot_2129}).
Slots and weekdays alone do not have as much information as the two combined. For example on a sunday evening, many people like to go to a restaurant and do not order, while during the week offices sometimes order big deliveries for lunch. The values in the diagram do not vary much, which can be due to the fact the training set has few values for some days.\newline
The difference between slots on weekdays as well as between weekdays is more significant than all other diagrams before and will be considered when creating the models.

\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{images/0000_boxWhisker_weekdaySlot_2129.png}
\caption{Box and whisker diagram on slot and weekday basis}
\label{fig:0000_boxWhisker_weekdaySlot_2129}
\end{center}
\end{figure}


\subsubsection{Restaurant Patterns}\label{subsection:Restaurant Wise Proceeding}
Preparation times can be very different by restaurant. There are restaurants, which have pre-cooked ingredients and others which prepare every meal fresh. Some have simple ways of preparing meals, like sandwiches, while others take long baking, frying or cooking. For all of them, preparation times are different. A roasted duck takes longer than a sandwich.\newline
This suggest looking at each restaurant separately, which initially leads to two categories: One restaurant agnostic and another one restaurant specific. For the evaluation of the restaurant specific forecast, yum2take is chosen as it is the most mature Volo customer with the most data points.\newline
In order to get a first look at the restaurant specific data, a diagram with slot and weekday differentiation is created in Figure \ref{fig:1_boxWhisker_weekdaySlot_686}.
\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{images/1_boxWhisker_weekdaySlot_686.png}
\caption{Observations of preparation time of all orders at yum2take without invalid values and outliers}
\label{fig:1_boxWhisker_weekdaySlot_686}
\end{center}
\end{figure}
The distribution in the diagram for yum2take is clearly different from the overall visualization. This confirms the assumption that restaurants should be looked at separately. The average times are also different. Instead of an average of 15 minutes for the preparation of food, the restaurant specific average time for yum2take is 13.1 minutes.
\subsection{Current Assumption}
As a stake in the ground and to compare future forecast models to the current process, the RMSE of the current forecasting mechanism where every preparation time is supposed to be 15 minutes is calculated. Based on the training data, it is 8.5 minutes. Since the improvement should be 33\%, the RMSE of a good forecast should be below 5.7 minutes.
\section{Choosing and Fitting Models}\label{Choosing and Fitting Models}
After the preliminary analysis, it is time to fit the data to models for the forecast. A model combines three dimensions, the algorithm, the restaurant category and a time category.
\subsection{Time Categorizing of Orders}\label{subsection:Categorizing by Order}
When categorized by time, only orders in a specific time window are used to calculate the forecast value for the current order. The time window is always relative to the current order. The following time categories were chosen:
\newline\newline\textbf{No Time Categorization}\newline
No time categorization means that for the forecast of the current order all predecessing orders are taken into account. This is done in order to treat all orders the same and get a simple forecast.
\newline\newline\textbf{Day Categorization}\newline
When using day categorization, only the orders before the current one on the same day are used to calculate the forecast value. The day categorization is done since from day to day conditions in the restaurant can change. For instance staff may be ill, delaying meals on that day.
\newline\newline\textbf{Week Categorization}\newline
The week categorization is done to take larger events into account, e.g. marketing campaigns. In order to consider such events, this categorization uses all predecessing orders of the calendar week for the calculation.
\newline\newline\textbf{Weekday Categorization}\newline
Weekdays are also different from each other and have to be considered since they cause different utilization of restaurants, e.g. on workdays there are big company lunch orders and on friday there are less. For this categorization all orders before the current on this weekday, regardless of the week, are used for the forecast.
\newline\newline\textbf{Slot Categorization}\newline
In the preliminary analysis the slot categorization was found relevant. These slots are lunch, dinner and the time in between and they cause different levels of traffic. Lunch and dinner being high traffic while the time in between is not so crowded. In order to consider these day specific differences, they can be applied together with the other time categorizations, to improve the accuracy of the forecast. For example when using day and slot categorization, not all predecessing orders of the current day are considered for the forecast but only the ones in the same slot and before the current order.
\newline\newline\textbf{Single Minislot Categorization}\newline
The minislot categorization divides the day into half hour slots. Since this granularity is finer than the slot categorization, it cannot be combined with it. For each forecast for an order all orders before the current one in this slot are used. This is done to consider short time fluctuations caused by peak times, e.g. the half hour in which most people do their lunch or shortly before eight in the evening before the prime time movie starts.\newline
Even though there might not be enough data for this categorization, it is done to see its potential.

\newline\newline\textbf{Edge Case}\newline
Before the actual forecast can be done, the edge case of not having predecessing orders for the current order, has to be resolved. Since the operations teams suggested a 15 minute basic time for preparation, this time is always taken when no forecast value can be generated for the current order.
\subsection{Combination of Time Categorizations}\label{subsection:Categorizing by Order}
Above categorizations can be combined to create a more complex forecasting model. Like the simple models, this complex model is created by using an algorithm and the restaurant category. Only this time a combination of multiple time categories is chosen instead of just one. Each time category is weighted, which is done in 5\% steps, e.g. 5\% category A, 20\% category B and 75\% for categories C-E, so all time categories together sum up to 100\%.  This is done for all possible combinations. In order to get a meaningful forecast, the most important time categories are chosen for the model. The time categories are:
\begin{enumerate}
\item Current slot
\item Current day
\item Current slot in the last 7 days
\item Last 7 days in general
\item Slot on the weekday in the last 4 weeks
\item Last 4 weeks in general
\end{enumerate}

The combination of different time categorizations is supposed to reflect short term trends, like marketing campaigns, weekly patterns, like busy sunday evenings, and seasonal patterns, like winter increasing the order amount.\newline
This combination is done to consider the behavior of orders in the light of all of these categorizations.
